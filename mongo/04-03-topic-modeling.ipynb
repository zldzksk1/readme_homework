{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from mongo_aggregation_verbs import *\n",
    "\n",
    "from lib import create_mongo_client_to_database_collection\n",
    "\n",
    "collection_reference = create_mongo_client_to_database_collection('twitter', 'tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://alexisperrier.com/nlp/2015/09/16/segmentation_twitter_timelines_lda_vs_lsa.html\n",
    "- https://alexisperrier.com/nlp/2015/09/04/topic-modeling-of-twitter-followers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 211518}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_empty_url_arrays = { MATCH : { \"entities.urls\" : [] } }\n",
    "\n",
    "list(collection_reference.aggregate(\n",
    "    [\n",
    "        match_empty_url_arrays,\n",
    "        { COUNT : \"text\" }\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'_id': ObjectId('5b568e378c4e2000a5220167')},\n",
       "  {'_id': ObjectId('5b569cc0987491012707420a')},\n",
       "  {'_id': ObjectId('5b569cc09874910127074228')},\n",
       "  {'_id': ObjectId('5b569cc09874910127074228')},\n",
       "  {'_id': ObjectId('5b569cc09874910127074228')},\n",
       "  {'_id': ObjectId('5b56a9261a6d22025b8bfd5b')},\n",
       "  {'_id': ObjectId('5b56a9a01a6d22025b8bff7f')},\n",
       "  {'_id': ObjectId('5b56a9ad1a6d22025b8bffbb')},\n",
       "  {'_id': ObjectId('5b56a9c81a6d22025b8c0047')},\n",
       "  {'_id': ObjectId('5b56aa001a6d22025b8c014d')}],\n",
       " 9904)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_hashtags = ['job', 'jobs', 'hiring', 'careerarc']\n",
    "location_hashtags = ['california', 'losangeles', 'la', 'santamonica', 'glendale', 'paloalto']\n",
    "match_not_in_bad = { MATCH : { \"text\" : { \"$in\" : job_hashtags + location_hashtags } } }\n",
    "project_to_text_keep_id = { PROJECT : { \"text\" : \"$entities.hashtags.text\" } }\n",
    "project_to_id = { PROJECT : { \"_id\" : 1 } }\n",
    "\n",
    "bad_ids = list(collection_reference.aggregate(\n",
    "    [\n",
    "        match_non_empty_hashtag_arrays,\n",
    "        project_to_text_keep_id,\n",
    "        unwind_text,\n",
    "        project_to_lower,\n",
    "        match_not_in_bad,\n",
    "        project_to_id\n",
    "    ]\n",
    "))\n",
    "bad_ids[:10], len(bad_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObjectId('5b568e378c4e2000a5220167'),\n",
       " ObjectId('5b569cc0987491012707420a'),\n",
       " ObjectId('5b569cc09874910127074228'),\n",
       " ObjectId('5b569cc09874910127074228'),\n",
       " ObjectId('5b569cc09874910127074228'),\n",
       " ObjectId('5b56a9261a6d22025b8bfd5b'),\n",
       " ObjectId('5b56a9a01a6d22025b8bff7f'),\n",
       " ObjectId('5b56a9ad1a6d22025b8bffbb'),\n",
       " ObjectId('5b56a9c81a6d22025b8c0047'),\n",
       " ObjectId('5b56aa001a6d22025b8c014d')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_ids = [bad_id['_id'] for bad_id in bad_ids]\n",
    "bad_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_bad_ids = { \"$nin\" : bad_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_bad_ids_and_no_url = { \n",
    "    \"_id\"           : not_in_bad_ids, \n",
    "    \"entities.urls\" : []\n",
    "}\n",
    "\n",
    "just_the_text = {\n",
    "    \"text\" : 1,\n",
    "    \"_id\"  : 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Can’t wait to go home and play video games already smh'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_reference.find_one(\n",
    "    not_in_bad_ids_and_no_url,\n",
    "    just_the_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur  = collection_reference.find(\n",
    "    not_in_bad_ids_and_no_url,\n",
    "    just_the_text\n",
    ")\n",
    "\n",
    "#tweets = list(cur)\n",
    "tweets = [next(cur) for _ in range(20000)] #너무 많아서 줄여줌\n",
    "tweet_text = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can’t wait to go home and play video games alr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can’t wait to go home and play video games alr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ya https://t.co/ZOjRvdvOFB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let’s go #Dodgers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Chunky peanut butter is rough on the teeth, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Can’t wait to go home and play video games alr...\n",
       "1  Can’t wait to go home and play video games alr...\n",
       "2                         ya https://t.co/ZOjRvdvOFB\n",
       "3                                  Let’s go #Dodgers\n",
       "4  “Chunky peanut butter is rough on the teeth, l..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_text.text = tweet_text.text.str.replace('http\\S+|www.\\S+', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf.fit(tweet_text.text)\n",
    "word_occurence = tfidf.transform(tweet_text.text) #.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_occurence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tfidf.get_feature_names()\n",
    "word_sample = random.sample(words, 20)\n",
    "word_occurence_m = pd.DataFrame(word_occurence, columns=words)\n",
    "word_occurence_m[word_sample].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD #TruncatedSVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda = LatentDirichletAllocation(n_topics=10, learning_method='batch')\n",
    "lda = TruncatedSVD(500)\n",
    "lda.fit(word_occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = pd.DataFrame(lda.components_, columns=words).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_topic(lda_df, index, threshold):\n",
    "    return (lda_df[lda_df[index] > threshold][index]\n",
    "            .sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_topic(lda_df, 7, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
